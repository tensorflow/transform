#
# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Tests for cached tf.Transform analysis."""

import functools
import os
import struct
import sys
import uuid
from typing import Callable, List, Mapping

import apache_beam as beam
import numpy as np
import tensorflow as tf
from apache_beam.testing import util as beam_test_util

# TODO(b/243513856): Switch to `collections.namedtuple` or `typing.NamedTuple`
# once the Spark issue is resolved.
from tfx_bsl.types import tfx_namedtuple

import tensorflow_transform as tft
import tensorflow_transform.beam as tft_beam
from tensorflow_transform import analyzer_nodes, common_types, impl_helper, nodes
from tensorflow_transform.beam import analysis_graph_builder, analyzer_cache, tft_unit
from tensorflow_transform.tf_metadata import dataset_metadata

mock = tf.compat.v1.test.mock

_SINGLE_PHASE_NUM_SAVED_MODELS = 2
_ZERO_PHASE_NUM_SAVED_MODELS = 1


def _make_cache_key(cache_identifier):
    return analyzer_cache._CACHE_VERSION + cache_identifier + b"-HASH"


def _encode_vocabulary_accumulator(token_bytes, value_bytes):
    return struct.pack(
        "qq{}s{}s".format(len(token_bytes), len(value_bytes)),
        len(token_bytes),
        len(value_bytes),
        token_bytes,
        value_bytes,
    )


def _preprocessing_fn_for_common_optimize_traversal(inputs):
    _ = tft.vocabulary(inputs["s"])
    x = inputs["x"]
    x_mean = tft.mean(x, name="x")
    x_square_deviations = tf.square(x - x_mean)

    # 2nd analysis phase defined here.
    x_var = tft.mean(x_square_deviations, name="x_square_deviations")
    x_normalized = (x - x_mean) / tf.sqrt(x_var)
    return {"x_normalized": x_normalized}


_OPTIMIZE_TRAVERSAL_COMMON_CASE = dict(
    testcase_name="common",
    feature_spec={
        "x": tf.io.FixedLenFeature([], tf.float32),
        "s": tf.io.FixedLenFeature([], tf.string),
    },
    preprocessing_fn=_preprocessing_fn_for_common_optimize_traversal,
    dataset_input_cache_dicts=[
        {
            _make_cache_key(b"CacheableCombineAccumulate[x#mean_and_var]"): "cache hit",
        }
    ],
    expected_dot_graph_str=r"""digraph G {
directed=True;
node [shape=Mrecord];
"CreateSavedModelForAnalyzerInputs[Phase0]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('vocabulary/boolean_mask/GatherV2', \"Tensor\<shape: [None], \<dtype: 'string'\>\>\"), ('x/mean_and_var/Cast_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x/mean_and_var/div_no_nan', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x/mean_and_var/div_no_nan_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x/mean_and_var/zeros', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\")])|label: CreateSavedModelForAnalyzerInputs[Phase0]}"];
"ExtractInputForSavedModel[AnalysisIndex0]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-0', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex0]}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"TensorSource[vocabulary][AnalysisIndex0]" [label="{ExtractFromDict|keys: ('vocabulary/boolean_mask/GatherV2',)|label: TensorSource[vocabulary][AnalysisIndex0]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" -> "TensorSource[vocabulary][AnalysisIndex0]";
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" [label="{VocabularyAccumulate|vocab_ordering_type: 1|input_dtype: string|label: VocabularyAccumulate[vocabulary][AnalysisIndex0]|partitionable: True}"];
"TensorSource[vocabulary][AnalysisIndex0]" -> "VocabularyAccumulate[vocabulary][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex1]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-1', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex1]}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"ExtractInputForSavedModel[AnalysisIndex1]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"TensorSource[vocabulary][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('vocabulary/boolean_mask/GatherV2',)|label: TensorSource[vocabulary][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[vocabulary][AnalysisIndex1]";
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" [label="{VocabularyAccumulate|vocab_ordering_type: 1|input_dtype: string|label: VocabularyAccumulate[vocabulary][AnalysisIndex1]|partitionable: True}"];
"TensorSource[vocabulary][AnalysisIndex1]" -> "VocabularyAccumulate[vocabulary][AnalysisIndex1]";
"FlattenCache[VocabularyMerge[vocabulary]]" [label="{Flatten|label: FlattenCache[VocabularyMerge[vocabulary]]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyMerge[vocabulary]" [label="{VocabularyMerge|vocab_ordering_type: 1|use_adjusted_mutual_info: False|min_diff_from_avg: None|label: VocabularyMerge[vocabulary]}"];
"FlattenCache[VocabularyMerge[vocabulary]]" -> "VocabularyMerge[vocabulary]";
"VocabularyCountUnfiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountUnfiltered[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyCountUnfiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_unpruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]}"];
"VocabularyCountUnfiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]";
"VocabularyPrune[vocabulary]" [label="{VocabularyPrune|top_k: None|frequency_threshold: 0|informativeness_threshold: -inf|coverage_top_k: None|coverage_frequency_threshold: 0|coverage_informativeness_threshold: -inf|key_fn: None|input_dtype: string|label: VocabularyPrune[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyPrune[vocabulary]";
"VocabularyCountFiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountFiltered[vocabulary]}"];
"VocabularyPrune[vocabulary]" -> "VocabularyCountFiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_pruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]}"];
"VocabularyCountFiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]";
"VocabularyOrderAndWrite[vocabulary]" [label="{VocabularyOrderAndWrite|vocab_filename: vocab_vocabulary|store_frequency: False|input_dtype: string|label: VocabularyOrderAndWrite[vocabulary]|fingerprint_shuffle: False|file_format: text|input_is_sorted: False}"];
"VocabularyPrune[vocabulary]" -> "VocabularyOrderAndWrite[vocabulary]";
"CreateTensorBinding[vocabulary#Placeholder]" [label="{CreateTensorBinding|tensor_name: vocabulary/Placeholder:0|dtype_enum: 7|is_asset_filepath: True|label: CreateTensorBinding[vocabulary#Placeholder]}"];
"VocabularyOrderAndWrite[vocabulary]" -> "CreateTensorBinding[vocabulary#Placeholder]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]|partitionable: True}"];
"TensorSource[x#mean_and_var][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('x/mean_and_var/Cast_1', 'x/mean_and_var/div_no_nan', 'x/mean_and_var/div_no_nan_1', 'x/mean_and_var/zeros')|label: TensorSource[x#mean_and_var][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[x#mean_and_var][AnalysisIndex1]";
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]|partitionable: True}"];
"TensorSource[x#mean_and_var][AnalysisIndex1]" -> "CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]";
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" [label="{Flatten|label: FlattenCache[CacheableCombineMerge[x#mean_and_var]]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineMerge[x#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x#mean_and_var]}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" -> "CacheableCombineMerge[x#mean_and_var]";
"ExtractCombineMergeOutputs[x#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x#mean_and_var]" -> "ExtractCombineMergeOutputs[x#mean_and_var]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":0 -> "CreateTensorBinding[x#mean_and_var#Placeholder]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":1 -> "CreateTensorBinding[x#mean_and_var#Placeholder_1]";
"CreateSavedModelForAnalyzerInputs[Phase1]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('x_square_deviations/mean_and_var/Cast_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/div_no_nan', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/div_no_nan_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/zeros', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\")])|label: CreateSavedModelForAnalyzerInputs[Phase1]}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase1]" [label="{ApplySavedModel|phase: 1|label: ApplySavedModel[Phase1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase1]" -> "ApplySavedModel[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase1]";
"TensorSource[x_square_deviations#mean_and_var]" [label="{ExtractFromDict|keys: ('x_square_deviations/mean_and_var/Cast_1', 'x_square_deviations/mean_and_var/div_no_nan', 'x_square_deviations/mean_and_var/div_no_nan_1', 'x_square_deviations/mean_and_var/zeros')|label: TensorSource[x_square_deviations#mean_and_var]|partitionable: True}"];
"ApplySavedModel[Phase1]" -> "TensorSource[x_square_deviations#mean_and_var]";
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x_square_deviations#mean_and_var]|partitionable: True}"];
"TensorSource[x_square_deviations#mean_and_var]" -> "CacheableCombineAccumulate[x_square_deviations#mean_and_var]";
"CacheableCombineMerge[x_square_deviations#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x_square_deviations#mean_and_var]}"];
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" -> "CacheableCombineMerge[x_square_deviations#mean_and_var]";
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x_square_deviations#mean_and_var]" -> "ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":0 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":1 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('x_normalized', \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\")])|label: CreateSavedModel}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" [label="{EncodeCache|coder: \<JsonNumpyCacheCoder\>|label: EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]|partitionable: True}"];
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" -> "EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]";
"EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" [label="{EncodeCache|coder: \<_VocabularyAccumulatorCoder\>|label: EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" -> "EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]";
"EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" [label="{EncodeCache|coder: \<_VocabularyAccumulatorCoder\>|label: EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" -> "EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]";
}
""",
    expected_dot_graph_str_py312=r"""digraph G {
directed=True;
node [shape=Mrecord];
"CreateSavedModelForAnalyzerInputs[Phase0]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'vocabulary/boolean_mask/GatherV2': \"Tensor\<shape: [None], \<dtype: 'string'\>\>\", 'x/mean_and_var/Cast_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x/mean_and_var/div_no_nan': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x/mean_and_var/div_no_nan_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x/mean_and_var/zeros': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModelForAnalyzerInputs[Phase0]}"];
"ExtractInputForSavedModel[AnalysisIndex0]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-0', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex0]}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"TensorSource[vocabulary][AnalysisIndex0]" [label="{ExtractFromDict|keys: ('vocabulary/boolean_mask/GatherV2',)|label: TensorSource[vocabulary][AnalysisIndex0]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" -> "TensorSource[vocabulary][AnalysisIndex0]";
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" [label="{VocabularyAccumulate|vocab_ordering_type: 1|input_dtype: string|label: VocabularyAccumulate[vocabulary][AnalysisIndex0]|partitionable: True}"];
"TensorSource[vocabulary][AnalysisIndex0]" -> "VocabularyAccumulate[vocabulary][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex1]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-1', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex1]}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"ExtractInputForSavedModel[AnalysisIndex1]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"TensorSource[vocabulary][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('vocabulary/boolean_mask/GatherV2',)|label: TensorSource[vocabulary][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[vocabulary][AnalysisIndex1]";
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" [label="{VocabularyAccumulate|vocab_ordering_type: 1|input_dtype: string|label: VocabularyAccumulate[vocabulary][AnalysisIndex1]|partitionable: True}"];
"TensorSource[vocabulary][AnalysisIndex1]" -> "VocabularyAccumulate[vocabulary][AnalysisIndex1]";
"FlattenCache[VocabularyMerge[vocabulary]]" [label="{Flatten|label: FlattenCache[VocabularyMerge[vocabulary]]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyMerge[vocabulary]" [label="{VocabularyMerge|vocab_ordering_type: 1|use_adjusted_mutual_info: False|min_diff_from_avg: None|label: VocabularyMerge[vocabulary]}"];
"FlattenCache[VocabularyMerge[vocabulary]]" -> "VocabularyMerge[vocabulary]";
"VocabularyCountUnfiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountUnfiltered[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyCountUnfiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_unpruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]}"];
"VocabularyCountUnfiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]";
"VocabularyPrune[vocabulary]" [label="{VocabularyPrune|top_k: None|frequency_threshold: 0|informativeness_threshold: -inf|coverage_top_k: None|coverage_frequency_threshold: 0|coverage_informativeness_threshold: -inf|key_fn: None|input_dtype: string|label: VocabularyPrune[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyPrune[vocabulary]";
"VocabularyCountFiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountFiltered[vocabulary]}"];
"VocabularyPrune[vocabulary]" -> "VocabularyCountFiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_pruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]}"];
"VocabularyCountFiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]";
"VocabularyOrderAndWrite[vocabulary]" [label="{VocabularyOrderAndWrite|vocab_filename: vocab_vocabulary|store_frequency: False|input_dtype: string|label: VocabularyOrderAndWrite[vocabulary]|fingerprint_shuffle: False|file_format: text|input_is_sorted: False}"];
"VocabularyPrune[vocabulary]" -> "VocabularyOrderAndWrite[vocabulary]";
"CreateTensorBinding[vocabulary#Placeholder]" [label="{CreateTensorBinding|tensor_name: vocabulary/Placeholder:0|dtype_enum: 7|is_asset_filepath: True|label: CreateTensorBinding[vocabulary#Placeholder]}"];
"VocabularyOrderAndWrite[vocabulary]" -> "CreateTensorBinding[vocabulary#Placeholder]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]|partitionable: True}"];
"TensorSource[x#mean_and_var][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('x/mean_and_var/Cast_1', 'x/mean_and_var/div_no_nan', 'x/mean_and_var/div_no_nan_1', 'x/mean_and_var/zeros')|label: TensorSource[x#mean_and_var][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[x#mean_and_var][AnalysisIndex1]";
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]|partitionable: True}"];
"TensorSource[x#mean_and_var][AnalysisIndex1]" -> "CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]";
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" [label="{Flatten|label: FlattenCache[CacheableCombineMerge[x#mean_and_var]]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineMerge[x#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x#mean_and_var]}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" -> "CacheableCombineMerge[x#mean_and_var]";
"ExtractCombineMergeOutputs[x#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x#mean_and_var]" -> "ExtractCombineMergeOutputs[x#mean_and_var]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":0 -> "CreateTensorBinding[x#mean_and_var#Placeholder]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":1 -> "CreateTensorBinding[x#mean_and_var#Placeholder_1]";
"CreateSavedModelForAnalyzerInputs[Phase1]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'x_square_deviations/mean_and_var/Cast_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/div_no_nan': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/div_no_nan_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/zeros': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModelForAnalyzerInputs[Phase1]}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase1]" [label="{ApplySavedModel|phase: 1|label: ApplySavedModel[Phase1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase1]" -> "ApplySavedModel[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase1]";
"TensorSource[x_square_deviations#mean_and_var]" [label="{ExtractFromDict|keys: ('x_square_deviations/mean_and_var/Cast_1', 'x_square_deviations/mean_and_var/div_no_nan', 'x_square_deviations/mean_and_var/div_no_nan_1', 'x_square_deviations/mean_and_var/zeros')|label: TensorSource[x_square_deviations#mean_and_var]|partitionable: True}"];
"ApplySavedModel[Phase1]" -> "TensorSource[x_square_deviations#mean_and_var]";
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x_square_deviations#mean_and_var]|partitionable: True}"];
"TensorSource[x_square_deviations#mean_and_var]" -> "CacheableCombineAccumulate[x_square_deviations#mean_and_var]";
"CacheableCombineMerge[x_square_deviations#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x_square_deviations#mean_and_var]}"];
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" -> "CacheableCombineMerge[x_square_deviations#mean_and_var]";
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x_square_deviations#mean_and_var]" -> "ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":0 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":1 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'x_normalized': \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModel}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" [label="{EncodeCache|coder: \<JsonNumpyCacheCoder\>|label: EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]|partitionable: True}"];
"CacheableCombineAccumulate[x#mean_and_var][AnalysisIndex1]" -> "EncodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]";
"EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" [label="{EncodeCache|coder: \<_VocabularyAccumulatorCoder\>|label: EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex0]" -> "EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]";
"EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" [label="{EncodeCache|coder: \<_VocabularyAccumulatorCoder\>|label: EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]|partitionable: True}"];
"VocabularyAccumulate[vocabulary][AnalysisIndex1]" -> "EncodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]";
}
""",
)

_OPTIMIZE_TRAVERSAL_MULTI_PHASE_FULL_CACHE_HIT_CASE = dict(
    testcase_name="multi_phase_full_cache_coverage",
    feature_spec={
        "x": tf.io.FixedLenFeature([], tf.float32),
        "s": tf.io.FixedLenFeature([], tf.string),
    },
    preprocessing_fn=_preprocessing_fn_for_common_optimize_traversal,
    dataset_input_cache_dicts=[
        {
            _make_cache_key(b"CacheableCombineAccumulate[x#mean_and_var]"): "cache hit",
            _make_cache_key(b"VocabularyAccumulate[vocabulary]"): "cache hit",
        }
    ]
    * 2,
    expected_dot_graph_str=r"""digraph G {
directed=True;
node [shape=Mrecord];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<_VocabularyAccumulatorCoder\>|label: DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]|partitionable: True}"];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-1', is_cached=True)|cache_key: \<bytes\>|coder: \<_VocabularyAccumulatorCoder\>|label: DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]|partitionable: True}"];
"FlattenCache[VocabularyMerge[vocabulary]]" [label="{Flatten|label: FlattenCache[VocabularyMerge[vocabulary]]|partitionable: True}"];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyMerge[vocabulary]" [label="{VocabularyMerge|vocab_ordering_type: 1|use_adjusted_mutual_info: False|min_diff_from_avg: None|label: VocabularyMerge[vocabulary]}"];
"FlattenCache[VocabularyMerge[vocabulary]]" -> "VocabularyMerge[vocabulary]";
"VocabularyCountUnfiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountUnfiltered[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyCountUnfiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_unpruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]}"];
"VocabularyCountUnfiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]";
"VocabularyPrune[vocabulary]" [label="{VocabularyPrune|top_k: None|frequency_threshold: 0|informativeness_threshold: -inf|coverage_top_k: None|coverage_frequency_threshold: 0|coverage_informativeness_threshold: -inf|key_fn: None|input_dtype: string|label: VocabularyPrune[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyPrune[vocabulary]";
"VocabularyCountFiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountFiltered[vocabulary]}"];
"VocabularyPrune[vocabulary]" -> "VocabularyCountFiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_pruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]}"];
"VocabularyCountFiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]";
"VocabularyOrderAndWrite[vocabulary]" [label="{VocabularyOrderAndWrite|vocab_filename: vocab_vocabulary|store_frequency: False|input_dtype: string|label: VocabularyOrderAndWrite[vocabulary]|fingerprint_shuffle: False|file_format: text|input_is_sorted: False}"];
"VocabularyPrune[vocabulary]" -> "VocabularyOrderAndWrite[vocabulary]";
"CreateTensorBinding[vocabulary#Placeholder]" [label="{CreateTensorBinding|tensor_name: vocabulary/Placeholder:0|dtype_enum: 7|is_asset_filepath: True|label: CreateTensorBinding[vocabulary#Placeholder]}"];
"VocabularyOrderAndWrite[vocabulary]" -> "CreateTensorBinding[vocabulary#Placeholder]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-1', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]|partitionable: True}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" [label="{Flatten|label: FlattenCache[CacheableCombineMerge[x#mean_and_var]]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineMerge[x#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x#mean_and_var]}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" -> "CacheableCombineMerge[x#mean_and_var]";
"ExtractCombineMergeOutputs[x#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x#mean_and_var]" -> "ExtractCombineMergeOutputs[x#mean_and_var]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":0 -> "CreateTensorBinding[x#mean_and_var#Placeholder]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":1 -> "CreateTensorBinding[x#mean_and_var#Placeholder_1]";
"CreateSavedModelForAnalyzerInputs[Phase1]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('x_square_deviations/mean_and_var/Cast_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/div_no_nan', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/div_no_nan_1', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"), ('x_square_deviations/mean_and_var/zeros', \"Tensor\<shape: [], \<dtype: 'float32'\>\>\")])|label: CreateSavedModelForAnalyzerInputs[Phase1]}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase1]" [label="{ApplySavedModel|phase: 1|label: ApplySavedModel[Phase1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase1]" -> "ApplySavedModel[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase1]";
"TensorSource[x_square_deviations#mean_and_var]" [label="{ExtractFromDict|keys: ('x_square_deviations/mean_and_var/Cast_1', 'x_square_deviations/mean_and_var/div_no_nan', 'x_square_deviations/mean_and_var/div_no_nan_1', 'x_square_deviations/mean_and_var/zeros')|label: TensorSource[x_square_deviations#mean_and_var]|partitionable: True}"];
"ApplySavedModel[Phase1]" -> "TensorSource[x_square_deviations#mean_and_var]";
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x_square_deviations#mean_and_var]|partitionable: True}"];
"TensorSource[x_square_deviations#mean_and_var]" -> "CacheableCombineAccumulate[x_square_deviations#mean_and_var]";
"CacheableCombineMerge[x_square_deviations#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x_square_deviations#mean_and_var]}"];
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" -> "CacheableCombineMerge[x_square_deviations#mean_and_var]";
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x_square_deviations#mean_and_var]" -> "ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":0 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":1 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('x_normalized', \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\")])|label: CreateSavedModel}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" -> CreateSavedModel;
}
""",
    expected_dot_graph_str_py312=r"""digraph G {
directed=True;
node [shape=Mrecord];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<_VocabularyAccumulatorCoder\>|label: DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]|partitionable: True}"];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-1', is_cached=True)|cache_key: \<bytes\>|coder: \<_VocabularyAccumulatorCoder\>|label: DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]|partitionable: True}"];
"FlattenCache[VocabularyMerge[vocabulary]]" [label="{Flatten|label: FlattenCache[VocabularyMerge[vocabulary]]|partitionable: True}"];
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex0]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"DecodeCache[VocabularyAccumulate[vocabulary]][AnalysisIndex1]" -> "FlattenCache[VocabularyMerge[vocabulary]]";
"VocabularyMerge[vocabulary]" [label="{VocabularyMerge|vocab_ordering_type: 1|use_adjusted_mutual_info: False|min_diff_from_avg: None|label: VocabularyMerge[vocabulary]}"];
"FlattenCache[VocabularyMerge[vocabulary]]" -> "VocabularyMerge[vocabulary]";
"VocabularyCountUnfiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountUnfiltered[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyCountUnfiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_unpruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]}"];
"VocabularyCountUnfiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]";
"VocabularyPrune[vocabulary]" [label="{VocabularyPrune|top_k: None|frequency_threshold: 0|informativeness_threshold: -inf|coverage_top_k: None|coverage_frequency_threshold: 0|coverage_informativeness_threshold: -inf|key_fn: None|input_dtype: string|label: VocabularyPrune[vocabulary]}"];
"VocabularyMerge[vocabulary]" -> "VocabularyPrune[vocabulary]";
"VocabularyCountFiltered[vocabulary]" [label="{VocabularyCount|label: VocabularyCountFiltered[vocabulary]}"];
"VocabularyPrune[vocabulary]" -> "VocabularyCountFiltered[vocabulary]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" [label="{CreateTensorBinding|tensor_name: vocabulary/vocab_vocabulary_pruned_vocab_size:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]}"];
"VocabularyCountFiltered[vocabulary]" -> "CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]";
"VocabularyOrderAndWrite[vocabulary]" [label="{VocabularyOrderAndWrite|vocab_filename: vocab_vocabulary|store_frequency: False|input_dtype: string|label: VocabularyOrderAndWrite[vocabulary]|fingerprint_shuffle: False|file_format: text|input_is_sorted: False}"];
"VocabularyPrune[vocabulary]" -> "VocabularyOrderAndWrite[vocabulary]";
"CreateTensorBinding[vocabulary#Placeholder]" [label="{CreateTensorBinding|tensor_name: vocabulary/Placeholder:0|dtype_enum: 7|is_asset_filepath: True|label: CreateTensorBinding[vocabulary#Placeholder]}"];
"VocabularyOrderAndWrite[vocabulary]" -> "CreateTensorBinding[vocabulary#Placeholder]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-0', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" [label="{DecodeCache|dataset_key: DatasetKey(key='span-1', is_cached=True)|cache_key: \<bytes\>|coder: \<JsonNumpyCacheCoder\>|label: DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]|partitionable: True}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" [label="{Flatten|label: FlattenCache[CacheableCombineMerge[x#mean_and_var]]|partitionable: True}"];
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex0]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"DecodeCache[CacheableCombineAccumulate[x#mean_and_var]][AnalysisIndex1]" -> "FlattenCache[CacheableCombineMerge[x#mean_and_var]]";
"CacheableCombineMerge[x#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x#mean_and_var]}"];
"FlattenCache[CacheableCombineMerge[x#mean_and_var]]" -> "CacheableCombineMerge[x#mean_and_var]";
"ExtractCombineMergeOutputs[x#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x#mean_and_var]" -> "ExtractCombineMergeOutputs[x#mean_and_var]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":0 -> "CreateTensorBinding[x#mean_and_var#Placeholder]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x#mean_and_var]":1 -> "CreateTensorBinding[x#mean_and_var#Placeholder_1]";
"CreateSavedModelForAnalyzerInputs[Phase1]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'x_square_deviations/mean_and_var/Cast_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/div_no_nan': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/div_no_nan_1': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\", 'x_square_deviations/mean_and_var/zeros': \"Tensor\<shape: [], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModelForAnalyzerInputs[Phase1]}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[vocabulary#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> "CreateSavedModelForAnalyzerInputs[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase1]" [label="{ApplySavedModel|phase: 1|label: ApplySavedModel[Phase1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase1]" -> "ApplySavedModel[Phase1]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase1]";
"TensorSource[x_square_deviations#mean_and_var]" [label="{ExtractFromDict|keys: ('x_square_deviations/mean_and_var/Cast_1', 'x_square_deviations/mean_and_var/div_no_nan', 'x_square_deviations/mean_and_var/div_no_nan_1', 'x_square_deviations/mean_and_var/zeros')|label: TensorSource[x_square_deviations#mean_and_var]|partitionable: True}"];
"ApplySavedModel[Phase1]" -> "TensorSource[x_square_deviations#mean_and_var]";
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" [label="{CacheableCombineAccumulate|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineAccumulate[x_square_deviations#mean_and_var]|partitionable: True}"];
"TensorSource[x_square_deviations#mean_and_var]" -> "CacheableCombineAccumulate[x_square_deviations#mean_and_var]";
"CacheableCombineMerge[x_square_deviations#mean_and_var]" [label="{CacheableCombineMerge|combiner: \<WeightedMeanAndVarCombiner\>|label: CacheableCombineMerge[x_square_deviations#mean_and_var]}"];
"CacheableCombineAccumulate[x_square_deviations#mean_and_var]" -> "CacheableCombineMerge[x_square_deviations#mean_and_var]";
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]" [label="{ExtractCombineMergeOutputs|output_tensor_info_list: [TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None), TensorInfo(dtype=tf.float32, shape=(), temporary_asset_info=None)]|label: ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]|{<0>0|<1>1}}"];
"CacheableCombineMerge[x_square_deviations#mean_and_var]" -> "ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":0 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]";
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x_square_deviations/mean_and_var/Placeholder_1:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]}"];
"ExtractCombineMergeOutputs[x_square_deviations#mean_and_var]":1 -> "CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'x_normalized': \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModel}"];
"CreateTensorBinding[vocabulary#vocab_vocabulary_unpruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#vocab_vocabulary_pruned_vocab_size]" -> CreateSavedModel;
"CreateTensorBinding[vocabulary#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#mean_and_var#Placeholder_1]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x_square_deviations#mean_and_var#Placeholder_1]" -> CreateSavedModel;
}
""",
)

_TF_VERSION_NAMED_PARAMETERS = [
    dict(testcase_name="CompatV1", use_tf_compat_v1=True),
    dict(testcase_name="V2", use_tf_compat_v1=False),
]


def _preprocessing_fn_for_generalized_chained_ptransforms(inputs):
    class FakeChainablePartitionable(
        tfx_namedtuple.namedtuple("FakeChainablePartitionable", ["label"]),
        nodes.OperationDef,
    ):
        def __new__(cls):
            scope = tf.compat.v1.get_default_graph().get_name_scope()
            label = "{}[{}]".format(cls.__name__, scope)
            return super(FakeChainablePartitionable, cls).__new__(cls, label=label)

        @property
        def num_outputs(self):
            return 1

        @property
        def is_partitionable(self):
            return True

    class FakeChainableCacheable(
        tfx_namedtuple.namedtuple("FakeChainableCacheable", ["label"]),
        nodes.OperationDef,
    ):
        def __new__(cls):
            scope = tf.compat.v1.get_default_graph().get_name_scope()
            label = "{}[{}]".format(cls.__name__, scope)
            return super(FakeChainableCacheable, cls).__new__(cls, label=label)

        @property
        def num_outputs(self):
            return 1

        @property
        def is_partitionable(self):
            return True

        @property
        def cache_coder(self):
            return "Not-a-coder-but-thats-ok!"

    class FakeChainable(
        tfx_namedtuple.namedtuple("FakeChainable", ["label"]), nodes.OperationDef
    ):
        def __new__(cls):
            scope = tf.compat.v1.get_default_graph().get_name_scope()
            label = "{}[{}]".format(cls.__name__, scope)
            return super(FakeChainable, cls).__new__(cls, label=label)

        @property
        def num_outputs(self):
            return 1

        @property
        def is_partitionable(self):
            return False

    with tf.compat.v1.name_scope("x"):
        input_values_node = nodes.apply_operation(
            analyzer_nodes.TensorSource, tensors=[inputs["x"]]
        )
        with tf.compat.v1.name_scope("partitionable1"):
            partitionable_outputs = nodes.apply_multi_output_operation(
                FakeChainablePartitionable, input_values_node
            )
        with tf.compat.v1.name_scope("cacheable1"):
            intermediate_cached_value_node = nodes.apply_multi_output_operation(
                FakeChainableCacheable, *partitionable_outputs
            )
        with tf.compat.v1.name_scope("partitionable2"):
            partitionable_outputs = nodes.apply_multi_output_operation(
                FakeChainablePartitionable, *intermediate_cached_value_node
            )
        with tf.compat.v1.name_scope("cacheable2"):
            cached_value_node = nodes.apply_multi_output_operation(
                FakeChainableCacheable, *partitionable_outputs
            )
        with tf.compat.v1.name_scope("partitionable3"):
            output_value_node = nodes.apply_multi_output_operation(
                FakeChainablePartitionable, *cached_value_node
            )
        with tf.compat.v1.name_scope("merge"):
            output_value_node = nodes.apply_operation(FakeChainable, *output_value_node)
        with tf.compat.v1.name_scope("not-cacheable"):
            non_cached_output = nodes.apply_operation(FakeChainable, input_values_node)
        x_chained = analyzer_nodes.bind_future_as_tensor(
            output_value_node, analyzer_nodes.TensorInfo(tf.float32, (17, 27), None)
        )
        x_plain = analyzer_nodes.bind_future_as_tensor(
            non_cached_output, analyzer_nodes.TensorInfo(tf.int64, (7, 13), None)
        )
        return {"x_chained": x_chained, "x_plain": x_plain}


_OPTIMIZE_TRAVERSAL_GENERALIZED_CHAINED_PTRANSFORMS_CASE = dict(
    testcase_name="generalized_chained_ptransforms",
    feature_spec={"x": tf.io.FixedLenFeature([], tf.float32)},
    preprocessing_fn=_preprocessing_fn_for_generalized_chained_ptransforms,
    dataset_input_cache_dicts=None,
    expected_dot_graph_str=r"""digraph G {
directed=True;
node [shape=Mrecord];
"CreateSavedModelForAnalyzerInputs[Phase0]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('inputs/inputs/x_copy', \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\")])|label: CreateSavedModelForAnalyzerInputs[Phase0]}"];
"ExtractInputForSavedModel[AnalysisIndex0]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-0', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex0]}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"TensorSource[x][AnalysisIndex0]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x][AnalysisIndex0]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" -> "TensorSource[x][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]|partitionable: True}"];
"TensorSource[x][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]";
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable1][AnalysisIndex0]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]" -> "FakeChainableCacheable[x/cacheable1][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]";
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable2][AnalysisIndex0]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]" -> "FakeChainableCacheable[x/cacheable2][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex1]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-1', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex1]}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"ExtractInputForSavedModel[AnalysisIndex1]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"TensorSource[x][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[x][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]|partitionable: True}"];
"TensorSource[x][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]";
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable1][AnalysisIndex1]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]" -> "FakeChainableCacheable[x/cacheable1][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]";
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable2][AnalysisIndex1]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]" -> "FakeChainableCacheable[x/cacheable2][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]";
"FlattenCache[FakeChainable[x/merge]]" [label="{Flatten|label: FlattenCache[FakeChainable[x/merge]]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]" -> "FlattenCache[FakeChainable[x/merge]]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]" -> "FlattenCache[FakeChainable[x/merge]]";
"FakeChainable[x/merge]" [label="{FakeChainable|label: FakeChainable[x/merge]}"];
"FlattenCache[FakeChainable[x/merge]]" -> "FakeChainable[x/merge]";
"CreateTensorBinding[x#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#Placeholder]}"];
"FakeChainable[x/merge]" -> "CreateTensorBinding[x#Placeholder]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase0]";
"TensorSource[x]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x]|partitionable: True}"];
"ApplySavedModel[Phase0]" -> "TensorSource[x]";
"FakeChainable[x/not-cacheable]" [label="{FakeChainable|label: FakeChainable[x/not-cacheable]}"];
"TensorSource[x]" -> "FakeChainable[x/not-cacheable]";
"CreateTensorBinding[x#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/Placeholder_1:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[x#Placeholder_1]}"];
"FakeChainable[x/not-cacheable]" -> "CreateTensorBinding[x#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict([('x_chained', \"Tensor\<shape: [17, 27], \<dtype: 'float32'\>\>\"), ('x_plain', \"Tensor\<shape: [7, 13], \<dtype: 'int64'\>\>\")])|label: CreateSavedModel}"];
"CreateTensorBinding[x#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#Placeholder_1]" -> CreateSavedModel;
"EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" -> "EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]";
"EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" -> "EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]";
"EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" -> "EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]";
"EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" -> "EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]";
InstrumentDatasetCache [label="{InstrumentDatasetCache|input_cache_dataset_keys: []|num_encode_cache: 4|num_decode_cache: 0|label: InstrumentDatasetCache|partitionable: True}"];
}
""",
    expected_dot_graph_str_py312=r"""digraph G {
directed=True;
node [shape=Mrecord];
"CreateSavedModelForAnalyzerInputs[Phase0]" [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'inputs/inputs/x_copy': \"Tensor\<shape: [None], \<dtype: 'float32'\>\>\"\})|label: CreateSavedModelForAnalyzerInputs[Phase0]}"];
"ExtractInputForSavedModel[AnalysisIndex0]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-0', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex0]}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex0]" -> "ApplySavedModel[Phase0][AnalysisIndex0]";
"TensorSource[x][AnalysisIndex0]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x][AnalysisIndex0]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex0]" -> "TensorSource[x][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]|partitionable: True}"];
"TensorSource[x][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]";
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable1][AnalysisIndex0]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex0]" -> "FakeChainableCacheable[x/cacheable1][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]";
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable2][AnalysisIndex0]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex0]" -> "FakeChainableCacheable[x/cacheable2][AnalysisIndex0]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" -> "FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]";
"ExtractInputForSavedModel[AnalysisIndex1]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='span-1', is_cached=True)|label: ExtractInputForSavedModel[AnalysisIndex1]}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0][AnalysisIndex1]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"ExtractInputForSavedModel[AnalysisIndex1]" -> "ApplySavedModel[Phase0][AnalysisIndex1]";
"TensorSource[x][AnalysisIndex1]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x][AnalysisIndex1]|partitionable: True}"];
"ApplySavedModel[Phase0][AnalysisIndex1]" -> "TensorSource[x][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]|partitionable: True}"];
"TensorSource[x][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]";
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable1][AnalysisIndex1]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable1][AnalysisIndex1]" -> "FakeChainableCacheable[x/cacheable1][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]";
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" [label="{FakeChainableCacheable|label: FakeChainableCacheable[x/cacheable2][AnalysisIndex1]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable2][AnalysisIndex1]" -> "FakeChainableCacheable[x/cacheable2][AnalysisIndex1]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]" [label="{FakeChainablePartitionable|label: FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" -> "FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]";
"FlattenCache[FakeChainable[x/merge]]" [label="{Flatten|label: FlattenCache[FakeChainable[x/merge]]|partitionable: True}"];
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex0]" -> "FlattenCache[FakeChainable[x/merge]]";
"FakeChainablePartitionable[x/partitionable3][AnalysisIndex1]" -> "FlattenCache[FakeChainable[x/merge]]";
"FakeChainable[x/merge]" [label="{FakeChainable|label: FakeChainable[x/merge]}"];
"FlattenCache[FakeChainable[x/merge]]" -> "FakeChainable[x/merge]";
"CreateTensorBinding[x#Placeholder]" [label="{CreateTensorBinding|tensor_name: x/Placeholder:0|dtype_enum: 1|is_asset_filepath: False|label: CreateTensorBinding[x#Placeholder]}"];
"FakeChainable[x/merge]" -> "CreateTensorBinding[x#Placeholder]";
"ExtractInputForSavedModel[FlattenedDataset]" [label="{ExtractInputForSavedModel|dataset_key: DatasetKey(key='FlattenedDataset', is_cached=True)|label: ExtractInputForSavedModel[FlattenedDataset]}"];
"ApplySavedModel[Phase0]" [label="{ApplySavedModel|phase: 0|label: ApplySavedModel[Phase0]|partitionable: True}"];
"CreateSavedModelForAnalyzerInputs[Phase0]" -> "ApplySavedModel[Phase0]";
"ExtractInputForSavedModel[FlattenedDataset]" -> "ApplySavedModel[Phase0]";
"TensorSource[x]" [label="{ExtractFromDict|keys: ('inputs/inputs/x_copy',)|label: TensorSource[x]|partitionable: True}"];
"ApplySavedModel[Phase0]" -> "TensorSource[x]";
"FakeChainable[x/not-cacheable]" [label="{FakeChainable|label: FakeChainable[x/not-cacheable]}"];
"TensorSource[x]" -> "FakeChainable[x/not-cacheable]";
"CreateTensorBinding[x#Placeholder_1]" [label="{CreateTensorBinding|tensor_name: x/Placeholder_1:0|dtype_enum: 9|is_asset_filepath: False|label: CreateTensorBinding[x#Placeholder_1]}"];
"FakeChainable[x/not-cacheable]" -> "CreateTensorBinding[x#Placeholder_1]";
CreateSavedModel [label="{CreateSavedModel|table_initializers: 0|output_signature: OrderedDict(\{'x_chained': \"Tensor\<shape: [17, 27], \<dtype: 'float32'\>\>\", 'x_plain': \"Tensor\<shape: [7, 13], \<dtype: 'int64'\>\>\"\})|label: CreateSavedModel}"];
"CreateTensorBinding[x#Placeholder]" -> CreateSavedModel;
"CreateTensorBinding[x#Placeholder_1]" -> CreateSavedModel;
"EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex0]" -> "EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex0]";
"EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable1][AnalysisIndex1]" -> "EncodeCache[FakeChainableCacheable[x/cacheable1]][AnalysisIndex1]";
"EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex0]" -> "EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex0]";
"EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]" [label="{EncodeCache|coder: Not-a-coder-but-thats-ok!|label: EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]|partitionable: True}"];
"FakeChainableCacheable[x/cacheable2][AnalysisIndex1]" -> "EncodeCache[FakeChainableCacheable[x/cacheable2]][AnalysisIndex1]";
InstrumentDatasetCache [label="{InstrumentDatasetCache|input_cache_dataset_keys: []|num_encode_cache: 4|num_decode_cache: 0|label: InstrumentDatasetCache|partitionable: True}"];
}
""",
)

_OPTIMIZE_TRAVERSAL_TEST_CASES = [
    _OPTIMIZE_TRAVERSAL_COMMON_CASE,
    _OPTIMIZE_TRAVERSAL_MULTI_PHASE_FULL_CACHE_HIT_CASE,
    _OPTIMIZE_TRAVERSAL_GENERALIZED_CHAINED_PTRANSFORMS_CASE,
]


def mock_out_cache_hash(test_fn):
    def _make_next_hashed_path_for_test(*unused_args):
        return b"HASH"

    def _run_test(*args, **kwargs):
        with mock.patch.object(
            analysis_graph_builder._OptimizeVisitor,
            "_make_next_hashed_path",
            _make_next_hashed_path_for_test,
        ):
            return test_fn(*args, **kwargs)

    return _run_test


_RunPipelineResult = tfx_namedtuple.namedtuple(  # pylint: disable=invalid-name
    "_RunPipelineResult", ["cache_output", "metrics", "transform_output"]
)


class CachedImplTest(tft_unit.TransformTestCase):
    def setUp(self):
        super().setUp()
        self.base_test_dir = os.path.join(
            os.environ.get("TEST_UNDECLARED_OUTPUTS_DIR", self.get_temp_dir()),
            self._testMethodName,
        )
        self._cache_dir = os.path.join(self.base_test_dir, "cache")
        self._running_index = 0

        self._context = tft_beam.Context(temp_dir=self.get_temp_dir())
        self._context.__enter__()

    def tearDown(self):
        self._context.__exit__()
        super().tearDown()

    def _get_running_index(self):
        self._running_index += 1
        return self._running_index

    def _publish_rendered_dot_graph_file_from_leaf_nodes(self, leaf_nodes):
        dot_string = nodes.get_dot_graph(leaf_nodes).to_string()
        tf.io.gfile.makedirs(self.base_test_dir)
        output_file = os.path.join(
            self.base_test_dir,
            "rendered_graph_{}.svg".format(self._get_running_index()),
        )
        self.WriteRenderedDotFile(dot_string, output_file=output_file)
        return dot_string

    def _publish_rendered_dot_graph_file(
        self,
        preprocessing_fn,
        feature_spec,
        dataset_keys,
        pcoll_cache_dict,
        use_tf_compat_v1=True,
    ):
        specs = feature_spec
        base_temp_dir = None
        if not use_tf_compat_v1:
            specs = impl_helper.get_type_specs_from_feature_specs(specs)
            base_temp_dir = self.base_test_dir
        graph, structured_inputs, structured_outputs = (
            impl_helper.trace_preprocessing_function(
                preprocessing_fn,
                specs,
                use_tf_compat_v1=use_tf_compat_v1,
                base_temp_dir=base_temp_dir,
            )
        )
        (transform_fn_future, cache_output_dict, sideeffects) = (
            analysis_graph_builder.build(
                graph,
                structured_inputs,
                structured_outputs,
                dataset_keys,
                pcoll_cache_dict,
            )
        )

        def sort_value_node_values(cache_dict):
            result = []
            if cache_dict is None:
                return result
            for dataset_cache in cache_dict.values():
                result.extend(dataset_cache.values())
            return sorted(result, key=str)

        return self._publish_rendered_dot_graph_file_from_leaf_nodes(
            [transform_fn_future]
            + sort_value_node_values(cache_output_dict)
            + list(sideeffects)
        )

    def _run_pipeline(
        self,
        feature_spec,
        input_data_dict,
        preprocessing_fn,
        cache_dict=None,
        should_read_cache=False,
        datasets_to_transform=None,
        expected_transform_data=None,
        expected_cache=None,
        transform_fn_output_dir="",
        use_tf_compat_v1=True,
    ) -> _RunPipelineResult:
        """Runs an analysis pipeline with cache.

        Args:
        ----
          feature_spec: A feature_spec for the input data.
          input_data_dict: Dict[str, List[Dict[str, primitive]]] the input data used
            for analysis.
          preprocessing_fn: The preprocessing_fn used for analysis.
          cache_dict: Dict[str, Dict[str, List[bytes]]], input cache dict. If
            provided, should_read_cache must be False.
          should_read_cache: A bool indicating if the pipeline should read cache. If
            True, cache_dict must be False.
          datasets_to_transform: List[str], list of dataset keys to transform.
          expected_transform_data: List[Dict[str, primitive]], the expected
            transformed data, should be the same for each dataset.
          expected_cache: Dict[str, Dict[str, bytes]], expected encoded cache.
          transform_fn_output_dir: A directory where the output transform_fn should
            be written to, if None provided it will not be written.
          use_tf_compat_v1: If True, TFT's public APIs (e.g. AnalyzeDataset) will
            use Tensorflow in compat.v1 mode. Defaults to `True`.

        Returns:
        -------
          A _RunPipelineResult.
        """
        input_metadata = dataset_metadata.DatasetMetadata.from_feature_spec(
            feature_spec
        )
        with self._TestPipeline() as p:
            with tft_beam.Context(force_tf_compat_v1=use_tf_compat_v1):
                # Wraps each value in input_data_dict as a PCollection.
                input_data_pcoll_dict = {}
                for a, b in input_data_dict.items():
                    pcoll = p | a.key >> beam.Create(b)
                    input_data_pcoll_dict[a] = pcoll

                pcoll_cache_dict = {}

                # If provided with a cache dictionary this wraps cache entries in
                # PCollections.
                if cache_dict is not None:
                    assert not should_read_cache
                    for dataset in cache_dict:
                        cache_entry = {}
                        for idx, (k, v) in enumerate(cache_dict[dataset].items()):
                            cache_entry[k] = (
                                p | f"CreateCache[{dataset}][{idx}]" >> beam.Create(v)
                            )
                        metadata = p | f"CreateCacheMetadata[{dataset}]" >> beam.Create(
                            [cache_dict[dataset].metadata]
                        )
                        pcoll_cache_dict[dataset] = analyzer_cache.DatasetCache(
                            cache_entry, metadata
                        )

                # If requested, reads cache from the test cache directory.
                if should_read_cache:
                    assert cache_dict is None
                    pcoll_cache_dict = p | analyzer_cache.ReadAnalysisCacheFromFS(
                        self._cache_dir, list(input_data_dict.keys())
                    )

                self._publish_rendered_dot_graph_file(
                    preprocessing_fn,
                    feature_spec,
                    set(input_data_dict.keys()),
                    pcoll_cache_dict,
                    use_tf_compat_v1=use_tf_compat_v1,
                )

                transform_fn, cache_output = (
                    input_data_pcoll_dict,
                    pcoll_cache_dict,
                    input_metadata,
                ) | "Analyze" >> tft_beam.AnalyzeDatasetWithCache(preprocessing_fn)
                _ = (
                    cache_output
                    | "WriteCache"
                    >> analyzer_cache.WriteAnalysisCacheToFS(p, self._cache_dir)
                )

                # Transforms the requested datasets.
                if datasets_to_transform is None:
                    transformed_dataset = None
                else:
                    flattened_transform_data = [
                        input_data_pcoll_dict[d] for d in datasets_to_transform
                    ] | "FlattenTransformData" >> beam.Flatten()
                    transformed_dataset = (
                        (flattened_transform_data, input_metadata),
                        transform_fn,
                    ) | "Transform" >> tft_beam.TransformDataset()

                # Validate the transformed data is as expected. This requires providing
                # datasets_to_transform.
                if expected_transform_data is not None:
                    assert transformed_dataset is not None
                    transformed_data, unused_transformed_metadata = transformed_dataset
                    beam_test_util.assert_that(
                        transformed_data,
                        beam_test_util.equal_to(expected_transform_data),
                    )

                if expected_cache is not None:
                    for dataset in expected_cache:
                        cache_dict = cache_output[dataset].cache_dict
                        self.assertCountEqual(
                            cache_dict.keys(), expected_cache[dataset].keys()
                        )
                        beam_test_util.assert_that(
                            cache_output[dataset].metadata,
                            beam_test_util.is_not_empty(),
                            label="AssertCacheMetadata[{}]".format(dataset),
                        )
                        for idx, (key, value) in enumerate(
                            expected_cache[dataset].items()
                        ):
                            beam_test_util.assert_that(
                                cache_dict[key],
                                beam_test_util.equal_to(value),
                                label="AssertCache[{}][{}]".format(dataset, idx),
                            )

                # Write transform_fn if provided with an output directory.
                tft_output = None
                if transform_fn_output_dir is not None:
                    if not transform_fn_output_dir:
                        transform_fn_output_dir = os.path.join(
                            self.base_test_dir, uuid.uuid4().hex
                        )
                    _ = transform_fn | tft_beam.WriteTransformFn(
                        transform_fn_output_dir
                    )
                    tft_output = tft.TFTransformOutput(transform_fn_output_dir)

        return _RunPipelineResult(cache_output, p.metrics, tft_output)

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    @mock_out_cache_hash
    def test_single_phase_mixed_analyzer_run_once(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")

        def preprocessing_fn(inputs):
            _ = tft.bucketize(inputs["x"], 2, name="bucketize")

            return {
                "integerized_s": tft.compute_and_apply_vocabulary(inputs["s"]),
                "x_min": tft.min(inputs["x"], name="x") + tf.zeros_like(inputs["x"]),
                "x_mean": tft.mean(inputs["x"], name="x") + tf.zeros_like(inputs["x"]),
                "y_min": tft.min(inputs["y"], name="y") + tf.zeros_like(inputs["y"]),
                "y_mean": tft.mean(inputs["y"], name="y") + tf.zeros_like(inputs["y"]),
            }

        # Run AnalyzeAndTransform on some input data and compare with expected
        # output.
        input_data = [{"x": 12, "y": 1, "s": "d"}, {"x": 10, "y": 1, "s": "c"}]
        feature_spec = {
            "x": tf.io.FixedLenFeature([], tf.float32),
            "y": tf.io.FixedLenFeature([], tf.float32),
            "s": tf.io.FixedLenFeature([], tf.string),
        }
        input_data_dict = {
            span_0_key: [
                {
                    "x": -2,
                    "y": 1,
                    "s": "b",
                },
                {
                    "x": 4,
                    "y": -4,
                    "s": "b",
                },
            ],
            span_1_key: input_data,
        }

        span_0_size = 42
        cache_dict = {
            span_0_key: analyzer_cache.DatasetCache(
                {
                    _make_cache_key(b"CacheableCombineAccumulate[x_1#mean_and_var]"): [
                        b"[2.0, 1.0, 9.0, 0.0]"
                    ],
                    _make_cache_key(b"CacheableCombineAccumulate[x#x]"): [
                        b"[2.0, 4.0]"
                    ],
                    _make_cache_key(b"CacheableCombineAccumulate[y_1#mean_and_var]"): [
                        b"[2.0, -1.5, 6.25, 0.0]"
                    ],
                    _make_cache_key(b"CacheableCombineAccumulate[y#y]"): [
                        b"[4.0, 1.0]"
                    ],
                },
                analyzer_cache.DatasetCacheMetadata(span_0_size),
            ),
            span_1_key: analyzer_cache.DatasetCache({}, None),
        }

        expected_transformed = [
            {
                "x_mean": 6.0,
                "x_min": -2.0,
                "y_mean": -0.25,
                "y_min": -4.0,
                "integerized_s": 1,
            },
            {
                "x_mean": 6.0,
                "x_min": -2.0,
                "y_mean": -0.25,
                "y_min": -4.0,
                "integerized_s": 2,
            },
        ]

        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            cache_dict=cache_dict,
            datasets_to_transform=[span_1_key],
            expected_transform_data=expected_transformed,
            transform_fn_output_dir=os.path.join(self.base_test_dir, "transform_fn"),
            use_tf_compat_v1=use_tf_compat_v1,
        )

        # The output cache should not have entries for the cache that is present
        # in the input cache.
        self.assertEqual(
            len(run_result.cache_output[span_0_key].cache_dict),
            len(run_result.cache_output[span_1_key].cache_dict) - 4,
        )

        metrics = run_result.metrics
        # 4 from analyzing 2 spans, and 2 from transform.
        self.assertMetricsCounterEqual(metrics, "num_instances", 6)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 4)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 8)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "num_packed_accumulate_combiners", 1)
        self.assertMetricsCounterEqual(metrics, "num_packed_merge_combiners", 1)
        # All datasets were processed even though some of the analyzers were covered
        # by cache.
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    def test_single_phase_run_twice(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")
        span_2_key = analyzer_cache.DatasetKey("span-2")

        def preprocessing_fn(inputs):
            _ = tft.vocabulary(inputs["s"], vocab_filename="vocab1")

            _ = tft.bucketize(inputs["x"], 2, name="bucketize")

            y_cov = tft.covariance(tf.expand_dims(inputs["y"], axis=1), tf.float32)
            return {
                "x_min": tft.min(inputs["x"], name="x") + tf.zeros_like(inputs["x"]),
                "x_mean": tft.mean(inputs["x"], name="x") + tf.zeros_like(inputs["x"]),
                "y_min": tft.min(inputs["y"], name="y") + tf.zeros_like(inputs["y"]),
                "y_mean": tft.mean(inputs["y"], name="y") + tf.zeros_like(inputs["y"]),
                "y_cov": tf.math.reduce_sum(y_cov) + tf.zeros_like(inputs["y"]),
                "s_integerized": tft.compute_and_apply_vocabulary(
                    inputs["s"], labels=inputs["label"], use_adjusted_mutual_info=True
                ),
            }

        feature_spec = {
            "x": tf.io.FixedLenFeature([], tf.float32),
            "y": tf.io.FixedLenFeature([], tf.float32),
            "s": tf.io.FixedLenFeature([], tf.string),
            "label": tf.io.FixedLenFeature([], tf.int64),
        }
        input_data_dict = {
            span_0_key: [],
            span_1_key: [
                {
                    "x": -2,
                    "y": 1,
                    "s": "a",
                    "label": 0,
                },
                {
                    "x": 4,
                    "y": -4,
                    "s": "a",
                    "label": 1,
                },
                {
                    "x": 5,
                    "y": 11,
                    "s": "a",
                    "label": 1,
                },
                {
                    "x": 1,
                    "y": -4,
                    "s": "ȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴".encode(),
                    "label": 1,
                },
            ],
            span_2_key: [
                {"x": 12, "y": 1, "s": "ȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴".encode(), "label": 0},
                {"x": 10, "y": 1, "s": "c", "label": 1},
            ],
        }
        expected_vocabulary_contents = np.array(
            [b"a", "ȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴".encode(), b"c"], dtype=object
        )

        expected_transformed_data = [
            {
                "x_mean": 5.0,
                "x_min": -2.0,
                "y_mean": 1.0,
                "y_min": -4.0,
                "y_cov": 25.0,
                "s_integerized": 0,
            },
            {
                "x_mean": 5.0,
                "x_min": -2.0,
                "y_mean": 1.0,
                "y_min": -4.0,
                "y_cov": 25.0,
                "s_integerized": 2,
            },
        ]

        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            datasets_to_transform=[span_2_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        for key in input_data_dict:
            self.assertIn(key, first_run_result.cache_output)
            self.assertEqual(8, len(first_run_result.cache_output[key].cache_dict))

        vocab1_path = first_run_result.transform_output.vocabulary_file_by_name(
            "vocab1"
        )
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        metrics = first_run_result.metrics
        # 6 from analyzing 3 spans, and 2 from transform.
        self.assertMetricsCounterEqual(metrics, "num_instances", 8)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        # 8 entries for each of 3 spans. Note that default values for the empty span
        # are also encoded.
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 24)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        second_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            datasets_to_transform=[span_2_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        vocab1_path = second_run_result.transform_output.vocabulary_file_by_name(
            "vocab1"
        )
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        self.assertFalse(second_run_result.cache_output)

        metrics = second_run_result.metrics
        # Only 2 from transform.
        self.assertMetricsCounterEqual(metrics, "num_instances", 2)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 24)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)

        # The root CreateSavedModel is optimized away because the data doesn't get
        # processed at all (only cache).
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _ZERO_PHASE_NUM_SAVED_MODELS
        )
        # Cache coverage allowed us to avoid processing this many bytes of data.
        self.assertMetricsCounterGreater(
            metrics, "analysis_input_bytes_from_cache", 413
        )

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    @mock_out_cache_hash
    def test_caching_vocab_for_integer_categorical(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")

        def preprocessing_fn(inputs):
            return {
                "x_vocab": tft.compute_and_apply_vocabulary(
                    inputs["x"], frequency_threshold=2
                )
            }

        feature_spec = {"x": tf.io.FixedLenFeature([], tf.int64)}
        input_data_dict = {
            span_0_key: [{"x": -2}, {"x": -4}, {"x": -1}, {"x": 4}],
            span_1_key: [{"x": -2}, {"x": -1}, {"x": 6}, {"x": 7}],
        }  # pyformat: disable
        expected_transformed_data = [
            {"x_vocab": 0},
            {"x_vocab": 1},
            {"x_vocab": -1},
            {"x_vocab": -1},
        ]  # pyformat: disable

        dataset_size = 17
        cache_dict = {
            span_0_key: analyzer_cache.DatasetCache(
                {
                    _make_cache_key(
                        b"VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]"
                    ): [
                        _encode_vocabulary_accumulator(b"-2", b"2"),
                        _encode_vocabulary_accumulator(b"-4", b"1"),
                        _encode_vocabulary_accumulator(b"-1", b"1"),
                        _encode_vocabulary_accumulator(b"4", b"1"),
                    ]
                },
                analyzer_cache.DatasetCacheMetadata(dataset_size=dataset_size),
            ),
            span_1_key: analyzer_cache.DatasetCache({}, None),
        }

        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            cache_dict=cache_dict,
            datasets_to_transform=[span_1_key],
            expected_transform_data=expected_transformed_data,
            transform_fn_output_dir=None,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        self.assertNotIn(span_0_key, run_result.cache_output)

        metrics = run_result.metrics
        # 4 from analysis since 1 span was completely cached, and 4 from transform.
        self.assertMetricsCounterEqual(metrics, "num_instances", 8)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 1)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 1)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        # Cache coverage allowed us to avoid processing this many bytes of data.
        self.assertMetricsCounterEqual(
            metrics, "analysis_input_bytes_from_cache", dataset_size
        )

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    @mock_out_cache_hash
    def test_non_frequency_vocabulary_merge(self, use_tf_compat_v1):
        """This test compares vocabularies produced with and without cache."""
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        mi_vocab_name = "mutual_information_vocab"
        adjusted_mi_vocab_name = "adjusted_mutual_information_vocab"
        weighted_frequency_vocab_name = "weighted_frequency_vocab"

        def preprocessing_fn(inputs):
            _ = tft.vocabulary(
                inputs["s"],
                labels=inputs["label"],
                store_frequency=True,
                vocab_filename=mi_vocab_name,
                min_diff_from_avg=0.1,
                use_adjusted_mutual_info=False,
                name="with_mi",
            )

            _ = tft.vocabulary(
                inputs["s"],
                labels=inputs["label"],
                store_frequency=True,
                vocab_filename=adjusted_mi_vocab_name,
                min_diff_from_avg=1.0,
                use_adjusted_mutual_info=True,
                name="with_adjusted_mi",
            )

            _ = tft.vocabulary(
                inputs["s"],
                weights=inputs["weight"],
                store_frequency=True,
                vocab_filename=weighted_frequency_vocab_name,
                use_adjusted_mutual_info=False,
                name="with_weight",
            )
            return inputs

        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")

        input_data = [
            dict(s="a", weight=1, label=1),
            dict(s="a", weight=0.5, label=1),
            dict(s="b", weight=0.75, label=1),
            dict(s="b", weight=1, label=0),
        ]
        feature_spec = {
            "s": tf.io.FixedLenFeature([], tf.string),
            "label": tf.io.FixedLenFeature([], tf.int64),
            "weight": tf.io.FixedLenFeature([], tf.float32),
        }
        input_data_dict = {
            span_0_key: input_data,
            span_1_key: input_data,
        }
        transform_fn_with_cache_dir = os.path.join(
            self.base_test_dir, "transform_fn_with_cache"
        )

        expected_accumulators = {
            _make_cache_key(b"VocabularyAccumulate[with_mi]"): [
                _encode_vocabulary_accumulator(
                    b"a", b"[2, [0.0, 1.0], [0.0, 0.0], 1.0]"
                ),
                _encode_vocabulary_accumulator(
                    b"b", b"[2, [0.5, 0.5], [0.0, 0.0], 1.0]"
                ),
                _encode_vocabulary_accumulator(
                    b"global_y_count_sentinel", b"[4, [0.25, 0.75], [0.0, 0.0], 1.0]"
                ),
            ],
            _make_cache_key(b"VocabularyAccumulate[with_adjusted_mi]"): [
                _encode_vocabulary_accumulator(
                    b"a", b"[2, [0.0, 1.0], [0.0, 0.0], 1.0]"
                ),
                _encode_vocabulary_accumulator(
                    b"b", b"[2, [0.5, 0.5], [0.0, 0.0], 1.0]"
                ),
                _encode_vocabulary_accumulator(
                    b"global_y_count_sentinel", b"[4, [0.25, 0.75], [0.0, 0.0], 1.0]"
                ),
            ],
            _make_cache_key(b"VocabularyAccumulate[with_weight]"): [
                _encode_vocabulary_accumulator(b"a", b"1.5"),
                _encode_vocabulary_accumulator(b"b", b"1.75"),
            ],
        }
        expected_cache = {
            span: expected_accumulators for span in [span_0_key, span_1_key]
        }

        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            transform_fn_output_dir=transform_fn_with_cache_dir,
            expected_cache=expected_cache,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        metrics = run_result.metrics
        # 4 from analysis on each of the input spans.
        self.assertMetricsCounterEqual(metrics, "num_instances", 8)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 6)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        with self._TestPipeline() as p:
            with tft_beam.Context():
                flat_data = p | "CreateInputData" >> beam.Create(input_data * 2)

                input_metadata = dataset_metadata.DatasetMetadata.from_feature_spec(
                    feature_spec
                )
                transform_fn_no_cache = (
                    flat_data,
                    input_metadata,
                ) | tft_beam.AnalyzeDataset(preprocessing_fn)

                transform_fn_no_cache_dir = os.path.join(
                    self.base_test_dir, "transform_fn_no_cache"
                )
                _ = transform_fn_no_cache | tft_beam.WriteTransformFn(
                    transform_fn_no_cache_dir
                )

        # 4 from analysis on each of the input spans.
        self.assertMetricsCounterEqual(p.metrics, "num_instances", 8)
        self.assertMetricsCounterEqual(p.metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(p.metrics, "cache_entries_encoded", 0)
        self.assertMetricsCounterEqual(
            p.metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(p.metrics, "analysis_input_bytes_from_cache", 0)

        tft_output_cache = tft.TFTransformOutput(transform_fn_with_cache_dir)
        tft_output_no_cache = tft.TFTransformOutput(transform_fn_no_cache_dir)

        for vocab_filename in (
            mi_vocab_name,
            adjusted_mi_vocab_name,
            weighted_frequency_vocab_name,
        ):
            cache_path = tft_output_cache.vocabulary_file_by_name(vocab_filename)
            no_cache_path = tft_output_no_cache.vocabulary_file_by_name(vocab_filename)
            with tf.io.gfile.GFile(cache_path, "rb") as f1, tf.io.gfile.GFile(
                no_cache_path, "rb"
            ) as f2:
                self.assertEqual(
                    f1.readlines(),
                    f2.readlines(),
                    "vocab with cache != vocab without cache for: {}".format(
                        vocab_filename
                    ),
                )

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    @mock_out_cache_hash
    def test_cached_ptransform_analyzer(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")

        class _AnalyzerMakeAccumulators(beam.PTransform):
            def expand(self, pcoll):
                input_sum = (
                    pcoll | beam.FlatMap(sum) | "ReduceSum" >> beam.CombineGlobally(sum)
                )
                size = (
                    pcoll
                    | beam.Map(np.size)
                    | "ReduceCount" >> beam.CombineGlobally(sum)
                )

                return (
                    pcoll.pipeline
                    | beam.Create([None])
                    | beam.Map(
                        lambda _, a, b: (a, b),  # pyformat: disable
                        beam.pvalue.AsSingleton(input_sum),
                        beam.pvalue.AsSingleton(size),
                    )
                )

        class _AnalyzerMergeAccumulators(beam.PTransform):
            def expand(self, pcoll):
                def merge(x):
                    zipped = list(zip(*x))
                    assert len(zipped) == 2, zipped
                    return sum(zipped[0]), sum(zipped[1])

                return pcoll | beam.CombineGlobally(merge)

        class _AnalyzerExtractOutput(beam.PTransform):
            def expand(self, pcoll):
                return pcoll | beam.Map(lambda p: p[0] / p[1])

        analyzer = tft.experimental.CacheablePTransformAnalyzer(
            make_accumulators_ptransform=_AnalyzerMakeAccumulators(),
            merge_accumulators_ptransform=_AnalyzerMergeAccumulators(),
            extract_output_ptransform=_AnalyzerExtractOutput(),
            cache_coder=tft.experimental.SimpleJsonPTransformAnalyzerCacheCoder(),
        )

        def preprocessing_fn(inputs):
            y = tft.experimental.ptransform_analyzer(
                [inputs["x"]], analyzer, [tf.int64], [[]]
            )
            return {"y": tf.zeros_like(inputs["x"]) + y}

        feature_spec = {"x": tf.io.FixedLenFeature([], tf.int64)}
        span_0_key = analyzer_cache.DatasetKey("span-0")
        input_data_dict = {span_0_key: [{"x": x} for x in range(7)]}
        expected_cache_dict = {
            span_0_key: {
                _make_cache_key(b"PTransform[ptransform#local_merge_accumulators]"): [
                    b"[21, 7]"
                ],
            },
        }
        expected_transformed_data = [{"y": 3} for _ in range(7)]
        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            datasets_to_transform=[span_0_key],
            expected_transform_data=expected_transformed_data,
            expected_cache=expected_cache_dict,
            use_tf_compat_v1=use_tf_compat_v1,
        )
        metrics = first_run_result.metrics
        # Incremented for both analysis and transform (7 * 2).
        self.assertMetricsCounterEqual(metrics, "num_instances", 14)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 1)
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            datasets_to_transform=[span_0_key],
            expected_transform_data=expected_transformed_data,
            expected_cache={},
            use_tf_compat_v1=use_tf_compat_v1,
        )
        metrics = first_run_result.metrics
        # This time analysis is skipped due to cache, only transform dataset counts.
        self.assertMetricsCounterEqual(metrics, "num_instances", 7)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 1)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)
        self.assertMetricsCounterGreater(
            metrics, "analysis_input_bytes_from_cache", 100
        )

    @tft_unit.named_parameters(*_OPTIMIZE_TRAVERSAL_TEST_CASES)
    @mock_out_cache_hash
    def test_optimize_traversal(
        self,
        feature_spec: Mapping[str, common_types.FeatureSpecType],
        preprocessing_fn: Callable[
            [Mapping[str, common_types.TensorType]],
            Mapping[str, common_types.TensorType],
        ],
        dataset_input_cache_dicts: List[Mapping[str, str]],
        expected_dot_graph_str: str,
        expected_dot_graph_str_py312: str,
    ):
        dataset_keys = [
            analyzer_cache.DatasetKey("span-0"),
            analyzer_cache.DatasetKey("span-1"),
        ]
        if dataset_input_cache_dicts is not None:
            cache = {
                key: analyzer_cache.DatasetCache(
                    cache_dict, analyzer_cache.DatasetCacheMetadata(1)
                )
                for key, cache_dict in zip(dataset_keys, dataset_input_cache_dicts)
            }  # pyformat: disable
        else:
            cache = {}
        dot_string = self._publish_rendered_dot_graph_file(
            preprocessing_fn, feature_spec, set(dataset_keys), cache
        )

        self.assertSameElements(
            (
                expected_dot_graph_str
                if sys.version_info < (3, 12)
                else expected_dot_graph_str_py312
            ).split("\n"),
            dot_string.split("\n"),
            msg="Result dot graph is:\n{}".format(dot_string),
        )

    def test_no_data_needed(self):
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")

        def preprocessing_fn(inputs):
            return {k: tf.identity(v) for k, v in inputs.items()}

        input_metadata = dataset_metadata.DatasetMetadata.from_feature_spec(
            {
                "x": tf.io.FixedLenFeature([], tf.float32),
            }
        )
        input_data_dict = {
            span_0_key: None,
            span_1_key: None,
        }

        with self._TestPipeline() as p:
            cache_dict = {
                span_0_key: {},
                span_1_key: {},
            }

            _, output_cache = (
                input_data_dict,
                cache_dict,
                input_metadata,
            ) | "Analyze" >> tft_beam.AnalyzeDatasetWithCache(
                preprocessing_fn, pipeline=p
            )
            self.assertFalse(output_cache)

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    def test_tf_function_works_with_cache(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")

        def preprocessing_fn(inputs, should_add_one):
            @tf.function
            def identity(x):
                if should_add_one:
                    x = x + 1
                return x

            return {
                "x_mean": tft.mean(identity(inputs["x"]), name="x")
                + tf.zeros_like(inputs["x"])
            }

        feature_spec = {"x": tf.io.FixedLenFeature([], tf.float32)}
        input_data_dict = {analyzer_cache.DatasetKey("span-0"): [dict(x=-2), dict(x=4)]}
        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            functools.partial(preprocessing_fn, should_add_one=False),
            transform_fn_output_dir=None,
            use_tf_compat_v1=use_tf_compat_v1,
        )
        first_cache_output, metrics = run_result.cache_output, run_result.metrics

        for key in input_data_dict:
            self.assertIn(key, first_cache_output)
            self.assertEqual(1, len(first_cache_output[key].cache_dict))

        self.assertMetricsCounterEqual(metrics, "num_instances", 2)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 1)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        # Cache is still valid since the contents of the tf.function are the same.
        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            functools.partial(preprocessing_fn, should_add_one=False),
            should_read_cache=True,
            transform_fn_output_dir=None,
            use_tf_compat_v1=use_tf_compat_v1,
        )
        second_cache_output, metrics = run_result.cache_output, run_result.metrics

        self.assertFalse(second_cache_output)

        self.assertMetricsCounterEqual(metrics, "num_instances", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 1)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _ZERO_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterGreater(metrics, "analysis_input_bytes_from_cache", 23)

        # Modifying the tf.function contents causes cache invalidation.
        run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            functools.partial(preprocessing_fn, should_add_one=True),
            should_read_cache=True,
            transform_fn_output_dir=None,
            use_tf_compat_v1=use_tf_compat_v1,
        )
        third_output_cache, metrics = run_result.cache_output, run_result.metrics

        for key in input_data_dict:
            self.assertIn(key, third_output_cache)
            self.assertEqual(1, len(third_output_cache[key].cache_dict))

        self.assertMetricsCounterEqual(metrics, "num_instances", 2)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 1)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

    def test_cache_with_missing_metadata(self):
        tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1")
        span_2_key = analyzer_cache.DatasetKey("span-2")

        def preprocessing_fn(inputs):
            return {
                "x_min": tft.min(inputs["x"], name="x") + tf.zeros_like(inputs["x"])
            }

        feature_spec = {"x": tf.io.FixedLenFeature([], tf.float32)}
        input_data_dict = {
            span_0_key: [],
            span_1_key: [{"x": idx} for idx in range(4)],
            span_2_key: [{"x": idx} for idx in range(2)],
        }  # pyformat: disable

        expected_transformed_data = [{"x_min": 0} for _ in range(2)]

        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            datasets_to_transform=[span_2_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=False,
        )

        # Deleting dataset cache metadata files.
        for key in input_data_dict:
            self.assertIn(key, first_run_result.cache_output)
            dataset_cache_dir = os.path.join(self._cache_dir, key.key)
            cache_metadata_files = tf.io.gfile.glob(
                os.path.join(
                    dataset_cache_dir, f"{analyzer_cache._METADATA_FILE_NAME}*"
                )
            )
            self.assertLen(cache_metadata_files, 1)
            os.rename(
                cache_metadata_files[0],
                os.path.join(
                    dataset_cache_dir, f"deleted_{analyzer_cache._METADATA_FILE_NAME}"
                ),
            )

        metrics = first_run_result.metrics
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 3)
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        second_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            datasets_to_transform=[span_2_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=False,
        )

        self.assertFalse(second_run_result.cache_output)

        metrics = second_run_result.metrics
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 3)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)
        # Because the metadata associated with cached datasets was deleted, we
        # report 0 for this counter.
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    def test_non_cached_dataset_run_twice(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")
        span_0_key = analyzer_cache.DatasetKey("span-0")
        span_1_key = analyzer_cache.DatasetKey("span-1").non_cacheable()

        num_analyzers = 2

        def preprocessing_fn(inputs):
            return {
                "x_min": tft.min(inputs["x"], name="x") + tf.zeros_like(inputs["x"]),
                "s_integerized": tft.compute_and_apply_vocabulary(
                    inputs["s"], vocab_filename="vocab"
                ),
            }

        feature_spec = {
            "x": tf.io.FixedLenFeature([], tf.float32),
            "s": tf.io.FixedLenFeature([], tf.string),
        }
        input_data_dict = {
            span_0_key: [
                {
                    "x": -2,
                    "s": "a",
                },
                {
                    "x": 4,
                    "s": "a",
                },
                {
                    "x": 5,
                    "s": "a",
                },
                {
                    "x": 1,
                    "s": "ȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴".encode(),
                },
            ],
            span_1_key: [
                {
                    "x": 5,
                    "s": "c",
                },
                {
                    "x": 5,
                    "s": "2",
                },
            ],
        }
        expected_vocabulary_contents = np.array(
            [b"a", "ȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴".encode(), b"c", b"2"], dtype=object
        )

        expected_transformed_data = [
            {
                "x_min": -2.0,
                "s_integerized": 2,
            },
            {
                "x_min": -2.0,
                "s_integerized": 3,
            },
        ]

        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            datasets_to_transform=[span_1_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        for key in input_data_dict:
            if key.is_cached:
                self.assertIn(key, first_run_result.cache_output)
                self.assertEqual(
                    num_analyzers, len(first_run_result.cache_output[key].cache_dict)
                )
            else:
                self.assertNotIn(key, first_run_result.cache_output)

        vocab1_path = first_run_result.transform_output.vocabulary_file_by_name("vocab")
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        metrics = first_run_result.metrics
        # 6 from analyzing 2 spans, and 2 from transform.
        self.assertMetricsCounterEqual(metrics, "num_instances", 8)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        # 2 analyzers, computed for just one dataset.
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", num_analyzers)
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        second_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            datasets_to_transform=[span_1_key],
            expected_transform_data=expected_transformed_data,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        vocab1_path = second_run_result.transform_output.vocabulary_file_by_name(
            "vocab"
        )
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        self.assertFalse(second_run_result.cache_output)

        metrics = second_run_result.metrics
        # Only 2 from transform, plus 2 from non-cache dataset.
        self.assertMetricsCounterEqual(metrics, "num_instances", 4)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", num_analyzers)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)

        # The non cached dataset causes the same number of saved models created as
        # in the first iteration.
        self.assertMetricsCounterEqual(
            metrics, "saved_models_created", _SINGLE_PHASE_NUM_SAVED_MODELS
        )
        # Cache coverage allowed us to avoid processing this many bytes of data.
        self.assertMetricsCounterGreater(
            metrics, "analysis_input_bytes_from_cache", 100
        )

    @tft_unit.parameters(
        dict(
            num_non_packed_analyzers=0,
            num_packed_analyzers=0,
            num_input_datasets=10,
            expected_node_count_with_cache=1,
            expected_node_count_without_cache=1,
        ),
        dict(
            num_non_packed_analyzers=2,
            num_packed_analyzers=0,
            num_input_datasets=1,
            expected_node_count_with_cache=29,
            expected_node_count_without_cache=24,
        ),
        dict(
            num_non_packed_analyzers=0,
            num_packed_analyzers=2,
            num_input_datasets=1,
            expected_node_count_with_cache=24,
            expected_node_count_without_cache=19,
        ),
        dict(
            num_non_packed_analyzers=2,
            num_packed_analyzers=0,
            num_input_datasets=10,
            expected_node_count_with_cache=101,
            expected_node_count_without_cache=24,
        ),
        dict(
            num_non_packed_analyzers=0,
            num_packed_analyzers=2,
            num_input_datasets=10,
            expected_node_count_with_cache=87,
            expected_node_count_without_cache=19,
        ),
    )
    def test_node_count(
        self,
        num_non_packed_analyzers,
        num_packed_analyzers,
        num_input_datasets,
        expected_node_count_with_cache,
        expected_node_count_without_cache,
    ):
        dataset_keys = [
            analyzer_cache.DatasetKey(str(x)) for x in range(num_input_datasets)
        ]
        specs = impl_helper.get_type_specs_from_feature_specs(
            {"x": tf.io.FixedLenFeature([], tf.int64)}
        )

        def preprocessing_fn(inputs):
            for _ in range(num_packed_analyzers):
                tft.mean(inputs["x"])
            for _ in range(num_non_packed_analyzers):
                tft.vocabulary(inputs["x"])
            return inputs

        def get_graph_leaf_nodes(cache_enabled):
            graph, structured_inputs, structured_outputs = (
                impl_helper.trace_preprocessing_function(
                    preprocessing_fn,
                    specs,
                    use_tf_compat_v1=False,
                    base_temp_dir=self.base_test_dir,
                )
            )
            (transform_fn_future, cache_output_dict, sideeffects) = (
                analysis_graph_builder.build(
                    graph,
                    structured_inputs,
                    structured_outputs,
                    dataset_keys,
                    cache_dict={} if cache_enabled else None,
                )
            )
            cache_output_nodes = []
            if cache_output_dict:
                for cache in cache_output_dict.values():
                    cache_output_nodes.extend(cache.values())
            return [transform_fn_future] + cache_output_nodes + list(sideeffects)

        with_cache_graph = get_graph_leaf_nodes(True)
        without_cache_graph = get_graph_leaf_nodes(False)
        node_count_with_cache = nodes.count_graph_nodes(with_cache_graph)
        node_count_without_cache = nodes.count_graph_nodes(without_cache_graph)
        self._publish_rendered_dot_graph_file_from_leaf_nodes(without_cache_graph)
        self._publish_rendered_dot_graph_file_from_leaf_nodes(with_cache_graph)
        self.assertEqual(
            (expected_node_count_without_cache, expected_node_count_with_cache),
            (node_count_without_cache, node_count_with_cache),
        )

    @tft_unit.named_parameters(_TF_VERSION_NAMED_PARAMETERS)
    def test_vocab_reserved_tokens_not_cached(self, use_tf_compat_v1):
        if not use_tf_compat_v1:
            tft_unit.skip_if_not_tf2("Tensorflow 2.x required.")

        def preprocessing_fn(inputs):
            return {
                "s_integerized": tft.compute_and_apply_vocabulary(
                    inputs["s"], vocab_filename="vocab", reserved_tokens=["reserved"]
                ),
            }

        feature_spec = {
            "s": tf.io.FixedLenFeature([], tf.string),
        }
        input_data_dict = {
            analyzer_cache.DatasetKey("span-0"): [dict(s="a")] * 3 + [dict(s="b")]
        }
        expected_vocabulary_contents = np.array([b"reserved", b"a", b"b"], dtype=object)

        first_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        vocab1_path = first_run_result.transform_output.vocabulary_file_by_name("vocab")
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        metrics = first_run_result.metrics
        self.assertMetricsCounterEqual(metrics, "num_instances", 4)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 1)
        self.assertMetricsCounterEqual(metrics, "analysis_input_bytes_from_cache", 0)

        second_run_result = self._run_pipeline(
            feature_spec,
            input_data_dict,
            preprocessing_fn,
            should_read_cache=True,
            use_tf_compat_v1=use_tf_compat_v1,
        )

        vocab1_path = second_run_result.transform_output.vocabulary_file_by_name(
            "vocab"
        )
        self.AssertVocabularyContents(vocab1_path, expected_vocabulary_contents)

        self.assertFalse(second_run_result.cache_output)

        metrics = second_run_result.metrics
        self.assertMetricsCounterEqual(metrics, "num_instances", 0)
        self.assertMetricsCounterEqual(metrics, "cache_entries_decoded", 1)
        self.assertMetricsCounterEqual(metrics, "cache_entries_encoded", 0)
        self.assertMetricsCounterGreater(metrics, "analysis_input_bytes_from_cache", 1)
